<document xmlns="http://cnx.rice.edu/cnxml">

<title>Use Image Signal Processing in Determining the Optimal Jumping Shots</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m58218</md:content-id>
  <md:title>Use Image Signal Processing in Determining the Optimal Jumping Shots</md:title>
  <md:abstract/>
  <md:uuid>b792b83d-8f65-4797-a4c8-584d5db9b94f</md:uuid>
</metadata>

<content>
  <section id="eip-129"><title>I. Introduction</title><para id="eip-265">The ever-improving computational prowess of mobile devices has enabled the combination of so many previously separated functions: the cell phone is now also a TV, a camera, a wallet full of credit cards, etc. We envisioned a phone camera that could go beyond simply capturing images and videos: with the help of back-end algorithms, this photography application will also be able to capture the image at the perfect moment of a person or multiple people jumping in the air.</para></section><section id="eip-617"><title>II. Motivation</title><para id="eip-658">More often than not, when we try to capture our excitement with a photo, we choose to capture the gesture that most effectively conveys the message: smiley face, hands in the air, feet off the ground. Yet we can only stay so long in mid-air that the perfectly-timed photo proves to be a hard task. That's why we develop this algorithm today, one that captures the perfect photo of the feet-off-the-ground ballerinas, cheerleaders, and supermen. On the other hand, we haven’t been able to find an application on either Apple App store or Android App store that does the similar trick. 
</para></section><section id="eip-496"><title>III. Goal</title><para id="eip-54">
The goal of this application is to capture the whole process of jumping and landing as a series of photos and then chooses the frame(s) where a certain reference point on the object (eg. foot) is at a certain coordinate (eg. highest off the ground) in the photo. Among the frames chose, this algorithms will again compare the quality of the image and choose the clearest image. The algorithm will cope with both one person jumping, capturing the clear frame that the feet are at the highest point of the ground, as well as multiple people jumping together, capturing the clear frame that all people’s distance from the group are well balanced and aligned. After the selection is accomplished, extra frames will be deleted to reduce RAM occupancy. Besides, we will integrate the algorithm into the embedded system of an ios device and reach maximum performance in terms of both memory and speed. 
</para></section><section id="eip-763"><title>IV. Potential Problem</title><list id="eip-917" list-type="enumerated" number-style="arabic"><item>The current algorithm that focuses on recognition might not be able to capture a rapidly moving target, thus potentially leaving important frames out of consideration.</item>
<item>Recognition might be distracted by unwanted objects within the frame.</item>
<item>Complicated image recognition algorithm might pose a challenge to the processing power of cell phone.</item></list></section><section id="eip-343"><title>V. Summary of Approach</title><para id="eip-854">
The input to our program could be either a photo stream or a video featuring a group of people jumping. If there are multiple people jumping in the image, we would also ask the user to specify the number of jumpers. First of all, the program would run face and upper body detection to locate the people in the image. However, since the face/body detection algorithm is not perfect, non-human components might be recognized as faces and real faces might go undetected. To deal with the problem, we have to perform a denoising process, which comes in two parts: the first would be to eliminate the background noise that is identified as faces (false positives), and the other would be to estimate the position of unrecognized faces (false negatives). With noise properly reduced, we would expect the program to recognize in each image exactly same number of faces as the number of jumpers specified by the user. Then, we would plot the position-time and velocity-time curves for everyone’s face, and selects the frame in which the jumping height is the highest and/or the velocity is nearly 0 as the best image. In the case that multiple people jump, we would recognize the frame in which everyone is in the air and their average jumping height is the largest as the optimal shot. The program selects the frame that everyone jumps the best i.e. each person is around the highest point in his/her projectile and there is no blur in the image. The back-end algorithms will continuously track the relative position of the person by locating his/her face and body within each frame and then calculates the relative displacement of the tracked object from frame to frame. As we know from Newton’s Law, an object at the peak of its projectile has 0 vertical velocity. In this case, a very small relative displacement of face/body indicates the possibility that the person is at the highest point of his projectile, thus suggesting such frame as a candidate for output.
</para></section><section id="eip-380"><title>VI. Details of Approach</title><para id="eip-133"><title>1. Face and body detection:</title>We first employed the Viola-Jones toolbox from Matlab to capture the face for each frame, but we found out both the false positives and false negatives are pretty high. Then we found similar problems while applying HOG feature detection for the upper body. However, there is a large overlap between the success rate in terms of frames for those two methods, that is, when face and upper body are detected in one region at the same time, that region is highly likely to contain a person; on the other hand, we understand that false positive are much harder to eliminate in the following denoising process while false negatives can be filled in by predicting the movements of the human body. As a result, we employed the algorithm that detect the face and human body at the same time, and we select the pair of y coordinates from both detections that are within a certain range, (assuming the person is jumping upright), then we simply disregard all the other detections.</para><para id="eip-448">Both Viola-Jones and HOG human body detection have adjustable threshold parameters to detect how strictly we recognize an object as a face or human body. We found out that higher threshold tends to produce better results if the background of the input is relatively complicated, and lower threshold tend to produce better results if the background of the input is clear and simple. For now, our algorithm has taken the threshold parameter as an input, we are planning on internalizing it so that the algorithm can automatically set the threshold based on the condition of the image. </para><para id="eip-848"><title>2. Denoising:</title>Denoising process involves correcting false positive and false negatives errors introduced by detection. Approaches to reduce false positives errors have been talked about in the previous section. To deal with false negatives, based on the assumption that jumping height changes quadratically with time we used quadratic regression curve to fit the height-time curve. If not all the faces are detected in a frame, we would use points predicted by the regression model as the height of the face in that frame, as illustrated by Figure. 1.  This way, we will be able to deduce the relative position of the face in each frame where the algorithms fails to locate the human face. We can also ensure that all the data points are available for the final selection process.</para><para id="eip-361"><figure id="Fitting">
  <title>Fitting</title>
  <media id="Fittingpic" alt="Fitting">
    <image mime-type="image/png" src="../../media/fig1.jpg"/>
  </media>
  <caption>
     Fitting missing data points at frames 4, 8, and 11
  </caption>
</figure></para><para id="eip-811"><title>3. Calibrate with reference to horizon:</title>The problem might occur that hand and camera shakes cause a person to be high in a frame, but this frame in fact is not where the person is jumping the highest. To capture the point where the person is truly at the highest point of his trajectory, we not only track the height of a person in the image, not also his absolute height, as measured with reference to the horizon. When the program runs, it performs constant background subtraction. If it is detected that background from frame to frame changes above a pre-selected threshold, which indicates that the camera is not fixed properly, the program will use the absolute height instead of the relative height in the image.</para><para id="eip-573"><title>4. Multiple jumpers:</title>In the case that more than one person jumps, at this stage we have to ask the user to specify how many people jump at the same time. Assuming that when a person jumps, his/her motion in the horizontal direction is small. Then, based on every person’s initial coordinates, we take face recognized at roughly the same horizontal position as the face that belongs to the same person. In this way, we separate each person’s jumping trajectory and will be able to track individually.</para><para id="eip-741"><title>5. Selection:</title>Once the previous steps are achieved, the selection process is easy: the algorithms will selected the index of the frame where the distance between the reference point and face is maximum. Figure. 3 illustrates the selected optimal picture when the program runs on the photo stream shown by Figure. 2.</para><para id="eip-731"><figure id="Stream">
  <title>Input Photo Stream</title>
  <media id="streampic" alt="Input Stream">
    <image mime-type="image/png" src="../../media/fig2-1333.jpg"/>
  </media>
  <caption>
     Input photo stream
  </caption>
</figure></para><para id="eip-391"><figure id="BestFrame">
  <title>Perfect Moment</title>
  <media id="pic" alt="frame pic taken">
    <image mime-type="image/png" src="../../media/fig3-e685.jpg"/>
  </media>
  <caption>
     Best frame selected.
  </caption>
</figure></para></section><section id="eip-131"><title>VII. Results:</title><para id="eip-403">
Judging from our demonstrations during the design process and the presentation session, we conclude that our application is able to fulfill its designed purpose of detecting one person with relatively short runtime （roughly 30 seconds for this prototype) . It is able to catch the frame that is within best 3% of the actual frames.  For multiple people detection, the algorithm works relatively after we specify how many people are jumping, and it can catch the frame within best 10% of actual frames. 
 

</para></section><section id="eip-580"><title>VIII. Future work：</title><para id="eip-542">
We have successfully constructed this algorithm to capture the perfect moment for a single person. Now we are working to enable this application to capture multiple people jumping at the same time. We are trying to tackle the following difficulties in future development:
</para><list id="eip-995" list-type="enumerated" number-style="arabic"><item>The algorithm needs to know how many faces to track and this input metrics is manually given right now. We will need work towards auto detection.</item>
<item>Multiple people might not jump at the same time, thus creating situations where the peaks of each projectile do not occur in the same frame. We are considering algorithms that could find the best possible relative position of faces within each frame.</item>
<item>As specified above, we have to set the threshold of the detection model as an input based on the complexity of the back ground. In the future, we are looking to let the algorithm detecting the complexity of the back ground and decide the threshold on its own. </item>
<item>We have designed our GUI for an IOS application already, and we found out open source packages of both Viola Jones algorithms and HOG feature detection algorithms online, so we are planning on integrating them together as a real smartphone application. 30 seconds are clearly not user-friendly enough, so we are definitely aiming for instant generation of results. </item>
<item>After talking with some audiences during the poster presentation, we are considering the possibility of real-time processing including a level of parallel computing and deep learning techniques for detecting faces more quickly. </item></list></section></content>

</document>